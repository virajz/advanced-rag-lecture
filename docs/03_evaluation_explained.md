# evaluation.py - The Report Card

## What is this file?

This file **grades** how well the RAG system is doing. It's like a teacher checking if the AI's answers are good!

## The Big Picture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   QUESTION   ‚îÇ     ‚îÇ   CONTEXTS   ‚îÇ     ‚îÇ    ANSWER    ‚îÇ
‚îÇ  "How do I   ‚îÇ     ‚îÇ  [Doc1,Doc2] ‚îÇ     ‚îÇ  "Use SSL    ‚îÇ
‚îÇ   setup SSL?"‚îÇ     ‚îÇ  Retrieved   ‚îÇ     ‚îÇ   certs..."  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                    ‚îÇ                    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   EVALUATE    ‚îÇ
                    ‚îÇ   üìä üìä üìä     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Context    ‚îÇ     ‚îÇ Faithfulness ‚îÇ     ‚îÇ   Answer     ‚îÇ
‚îÇ  Relevance   ‚îÇ     ‚îÇ              ‚îÇ     ‚îÇ  Relevance   ‚îÇ
‚îÇ    0.67      ‚îÇ     ‚îÇ    0.80      ‚îÇ     ‚îÇ    0.50      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## The Three Grades (Metrics)

### 1. Context Relevance üìö
**Question:** "Did we find the RIGHT documents?"

```python
def context_relevance(question: str, contexts) -> float:
```

- Takes the question words and checks: do the retrieved documents contain these words?
- Score: 0.0 (terrible) to 1.0 (perfect)

**Example:**
- Question: "How do I configure SSL?"
- If 2 out of 3 retrieved docs mention "SSL" or "configure" ‚Üí Score = 0.67

### 2. Faithfulness ü§ù
**Question:** "Is the answer actually based on the documents?"

```python
def faithfulness(answer: str, contexts) -> float:
```

- Checks if the answer's claims can be found in the retrieved documents
- Prevents the AI from making stuff up!

**Example:**
- Answer: "Use PEM files. Rotate every 90 days."
- If both claims appear in the documents ‚Üí High score!
- If the AI says something not in the docs ‚Üí Lower score

### 3. Answer Relevance ‚úÖ
**Question:** "Does the answer actually address the question?"

```python
def answer_relevance(question: str, answer: str) -> float:
```

- Checks word overlap between question and answer
- Makes sure we're not answering a different question!

## Explained Like You're 5

Imagine you ask your friend: **"What's your favorite color?"**

| Metric | Good Example | Bad Example |
|--------|--------------|-------------|
| Context Relevance | Friend looks at a color chart | Friend looks at a food menu |
| Faithfulness | "Blue - it says here blue is calming" | "Blue - I just made that up" |
| Answer Relevance | "My favorite color is blue" | "I had pizza for lunch" |

## The Main Function

```python
def evaluate(question: str, contexts, answer: str):
    return {
        "context_relevance": context_relevance(question, contexts),
        "faithfulness": faithfulness(answer, contexts),
        "answer_relevance": answer_relevance(question, answer),
    }
```

This bundles all three scores into one report card!

## Important Note ‚ö†Ô∏è

This is a **simplified** evaluation! The comments say "rough" and "very rough" because:
- Real evaluation uses AI models to judge
- These just count word overlaps
- It's good for learning, not production!

## Example Output

```python
{
    "context_relevance": 0.67,  # 67% of docs were relevant
    "faithfulness": 0.80,       # 80% of claims were grounded
    "answer_relevance": 0.50    # 50% word overlap with question
}
```

---
*This file helps you understand: "Is my RAG system actually working well?"*
